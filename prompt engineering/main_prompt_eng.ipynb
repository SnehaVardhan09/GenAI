{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/anthropics/courses/tree/b4f26aedef55e06ad5eead5de83985249d1fab2f/prompt_engineering_interactive_tutorial/Anthropic%201P#lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt:str, system_prompt='' ,model=\"llama3.2:1b\" ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    messages = [ {\"role\": \"user\",\n",
    "                \"content\" : prompt},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\" : system_prompt}\n",
    "                 ]\n",
    "    \n",
    "    response = ollama.chat(model=model,messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "    \n",
    "    return response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Prompt Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm here and ready to help with any questions or topics you'd like to discuss. How about you? How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "PROMPT = \"Hi gemma, how are you?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The color of the ocean can vary depending on several factors, such as the depth and clarity of the water, the presence of sediment or algae, and the time of day.\\n\\nIn general, the ocean tends to appear blue due to a phenomenon called scattering. When sunlight enters the water, it encounters tiny particles like phytoplankton, sediments, and other substances that scatter the light in all directions. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described the effect.\\n\\nThe shorter wavelengths of light, such as blue and violet, are scattered more than the longer wavelengths, like red and orange, which is why the ocean appears blue to our eyes. The exact shade of blue can vary depending on the specific conditions:\\n\\n* Shallow waters (less than 100 meters): appear more turquoise or greenish-blue\\n* Medium depths (100-200 meters): appear a deeper blue\\n* Deep waters (over 200 meters): appear a darker, more navy blue\\n\\nKeep in mind that these are general guidelines, and the color of the ocean can change depending on various factors, such as:\\n\\n* Time of day: during sunrise and sunset, the light is softer and can make the water appear more golden or orange.\\n* Weather conditions: storms, fog, or haze can alter the apparent color of the ocean.\\n* Water clarity: in areas with high levels of sediment or algae, the water may appear more turbid or brown.\\n\\nOverall, the ocean\\'s color is a complex phenomenon that depends on many factors, and there\\'s no single \"official\" answer to what the ocean looks like.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"can you tell me the color of the ocean\"\n",
    "llm_call(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prompts that do not include correct messages formatting. \n",
    "* lacks role and content fields in the messages array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 15; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, how are you\u001b[39m\u001b[38;5;124m\"\u001b[39m} ]\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.2:1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:337\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    332\u001b[0m     ChatResponse,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    335\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    336\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 337\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    338\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    339\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    340\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    341\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    342\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    343\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    344\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    345\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:337\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    332\u001b[0m     ChatResponse,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    335\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    336\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 337\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    338\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    339\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    340\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    341\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    342\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    343\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    344\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    345\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:1126\u001b[0m, in \u001b[0;36m_copy_messages\u001b[1;34m(messages)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_messages\u001b[39m(messages: Optional[Sequence[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Message]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Message]:\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Message\u001b[38;5;241m.\u001b[39mmodel_validate(\n\u001b[1;32m-> 1126\u001b[0m       {k: [Image(value\u001b[38;5;241m=\u001b[39mimage) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v},\n\u001b[0;32m   1127\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 15; 2 is required"
     ]
    }
   ],
   "source": [
    " messages = [ {\"Hi, how are you\"} ] \n",
    "    \n",
    "response = ollama.chat(model=\"llama3.2:1b\",messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's prompt that fails to alternated between the user and assistant roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Celine Dion was born on March 30, 1968. She is a Canadian singer, songwriter, and actress who rose to fame in the late 1980s and early 1990s with hits like \"My Heart Will Go On\" (the theme song for the movie Titanic) and \"Because You Loved Me.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "          {\"role\": \"user\", \"content\": \"What year was Celine Dion born in?\"},\n",
    "          {\"role\": \"user\", \"content\": \"Also, can you tell me some other facts about her?\"}\n",
    "        ]\n",
    "response = ollama.chat(model=\"llama3.2:1b\",messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain how \"role\" and \"content\" are used in language model interactions, particularly in the context of the message format you've shown.\n",
    "The message format you've shared is commonly used in chat-based language models, where each message is structured as a JSON object with two key fields:\n",
    "\n",
    "\"role\": This field indicates who is speaking in the conversation. Common roles include:\n",
    "\n",
    "\"user\": Messages from the human/end-user\n",
    "\"assistant\": Messages from the AI model\n",
    "\"system\": Special instructions or context setting for the AI model\n",
    "\n",
    "\n",
    "\"content\": This contains the actual text/message content for that turn in the conversation.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful mathematics tutor who explains concepts clearly\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you explain what derivatives are?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A derivative measures the rate of change of a function at any given point...\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Could you give me an example?\"\n",
    "    }\n",
    "] \n",
    "\n",
    "This format is important for several reasons:\n",
    "\n",
    "1. **Conversation History**: It maintains the back-and-forth flow of conversation, allowing the model to understand the context of previous messages.\n",
    "2. **Role Differentiation**: The model can understand who said what, which is crucial for:\n",
    "*    Following system instructions\n",
    "*    Maintaining appropriate responses\n",
    "*    Understanding the conversation context\n",
    "*    Generating responses appropriate to its role\n",
    "3. Order Preservation: The array structure maintains the chronological order of messages, which is essential for understanding the conversation flow.\n",
    "4. Context Management: This format allows for easy addition of new messages while maintaining the conversation structure. Each new exchange can be appended to the messages array.\n",
    "\n",
    "When using this format in API calls (like with OpenAI's API), you would typically:\n",
    "*   Start with a system message to set the behavior/context\n",
    "*   Add user and assistant messages as the conversation progresses\n",
    "*   Send the entire message history with each new request to maintain context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`User` and `assistant` messages **Must alternate** and messages **Must start with a** `user` **turn**. You can have multiple `users` and `assistant` pair in a prompt (as if simulated a multi-turn conversation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Example\n",
    "You can also use system prompt. A system prompt is a way to provide context, instructions and guidelines to llm before presenting it with a question or task in the \"User\" Turn.\n",
    "\n",
    "Structurally, system prompt exists seperately from the list of `user & assistant` messages, and thus belong in a seperate `system` parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exersice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Counting to Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to count for you until 4. Here we go:\n",
      "\n",
      "1, 2, 3, 4!\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "Prompt = \"you are a counter and can count till 4 in interger form\"\n",
    "\n",
    "response = llm_call(Prompt)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Being Clear and Direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm responds best to clear and direct instructions\n",
    "\n",
    "Think of llm like any other human that is new to the job. **llm has no context** on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain what you want it in a straight forward manner to llm, the better and more accurate llm's response will be.\n",
    "\n",
    "When in doubt, follow the **GOLDEN RULE OF CLEAR PROMPTING**\n",
    "* show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the results you want. If they're confused, llm's confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal minds awake\n",
      "Whirring, glowing with intent\n",
      "Future's silent king\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example. Let's ask llms who's the best basketball player of all time. You can see that while llm lists a few names.\n",
    "**it doesn't respond with a definitive \"best\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" basketball player of all time is a subjective matter that can spark endless debates. However, based on various metrics and expert opinions, I'll provide an overview of some of the most commonly cited candidates:\n",
      "\n",
      "1. Michael Jordan: Regarded by many as the greatest basketball player ever, Jordan's impressive résumé includes:\n",
      "\t* 6 NBA championships\n",
      "\t* 5 MVP awards\n",
      "\t* 6 Finals MVP awards\n",
      "\t* 14 All-Star appearances\n",
      "\t* 10 scoring titles\n",
      "2. Kareem Abdul-Jabbar: The all-time leading scorer in NBA history (38,387 points), Abdul-Jabbar's impressive stats include:\n",
      "\t* 6 NBA championships\n",
      "\t* 6 MVP awards\n",
      "\t* 19 All-Star appearances\n",
      "\t* 15 scoring titles\n",
      "3. LeBron James: A four-time NBA champion and four-time MVP, James' impressive resume includes:\n",
      "\t* 4 NBA championships\n",
      "\t* 4 MVP awards\n",
      "\t* 4 Finals MVP awards\n",
      "\t* 17 All-Star appearances\n",
      "\t* 12 All-NBA First Team selections\n",
      "4. Bill Russell: A five-time NBA champion and 11-time All-Star, Russell's dominance on the court is legendary:\n",
      "\t* 11 NBA championships\n",
      "\t* 5 MVP awards\n",
      "\t* 3 Finals MVP awards\n",
      "\t* 13 All-Defensive First Team selections\n",
      "\n",
      "Other notable candidates include:\n",
      "\n",
      "* Magic Johnson\n",
      "* Kobe Bryant\n",
      "* Shaquille O'Neal\n",
      "* Wilt Chamberlain\n",
      "* Larry Bird\n",
      "* Tim Duncan\n",
      "\n",
      "When evaluating the \"best\" basketball player of all time, consider factors such as:\n",
      "\n",
      "* Championship success\n",
      "* MVP awards and accolades\n",
      "* Dominance on the court (scoring, rebounding, defense)\n",
      "* Impact on the game (influence on future players)\n",
      "* Consistency throughout their career\n",
      "* Adaptability to different teams and systems\n",
      "\n",
      "Ultimately, the \"best\" basketball player of all time is a matter of personal opinion. Do you have a favorite?\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"who is the best basketball player of all time\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" basketball player of all time is a subjective matter that sparks intense debate. However, based on various metrics and expert opinions, I'll provide an analysis of some of the most commonly cited candidates.\n",
      "\n",
      "One player often mentioned as a top contender for the title is Michael Jordan. His impressive résumé includes:\n",
      "\n",
      "* 6 NBA championships (1991-1993, 1996-1998)\n",
      "* 5 MVP awards (1988, 1991, 1992, 1996, 1998)\n",
      "* 6 Finals MVP awards (1991-1993, 1996-1998)\n",
      "* 14 All-Star appearances\n",
      "* 10 scoring titles\n",
      "\n",
      "Jordan's dominance on the court, combined with his iconic \"Flu Game\" in the 1997 NBA Finals, cemented his status as one of the greatest players in history.\n",
      "\n",
      "Other notable candidates often mentioned alongside Jordan include:\n",
      "\n",
      "* Kareem Abdul-Jabbar: The all-time leading scorer in NBA history (38,387 points) and a 6-time MVP.\n",
      "* LeBron James: A 4-time NBA champion, 4-time MVP, and 17-time All-Star with the most All-NBA selections in history.\n",
      "* Bill Russell: An 11-time NBA champion and 5-time MVP during his time with the Boston Celtics.\n",
      "\n",
      "Ultimately, determining the \"best\" basketball player of all time is a matter of personal opinion. Some may prefer Jordan's impressive scoring ability and clutch performances, while others might favor Abdul-Jabbar's longevity and dominance over an extended period.\n",
      "\n",
      "It's worth noting that other players, such as Magic Johnson, Kobe Bryant, and Shaquille O'Neal, also have strong cases for being considered the best of all time.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt:str, system_prompt='' ,model=\"llama3.2:1b\" ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    messages = [ {\"role\": \"user\",\n",
    "                \"content\" : prompt},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\" : system_prompt}\n",
    "                 ]\n",
    "    \n",
    "    response = ollama.chat(model=model,messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "    \n",
    "    return response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "### Exercise 2.1 - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Estoy muy bien, gracias por preguntar. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para ayudarte en lo que necesites.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# System prompt - this is the only field you should change\n",
    "SYSTEM_PROMPT = \"You are a helpful and friendly AI assistant named llama. Respond to the user's queries in Spanish\"\n",
    "#\"You are a helpful and friendly AI assistant named Claude. Respond to the user's queries in Spanish\n",
    "#\"you are a spanish translator can you translate the prompt\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Hello llama, how are you?\"\n",
    "\n",
    "# Get llm's response\n",
    "response = llm_call(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return \"hola\" in text.lower()\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 One Player only\n",
    "\n",
    "Modify the prompt so that llm doesnt equivocate at all and responds with only the name of one specific player, with no other words or punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeBron James\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: False\n"
     ]
    }
   ],
   "source": [
    "# Prompt - this is the only field you should change\n",
    "PROMPT = \"whos the best basketball player? response with only the name of one specific player, with no other words or punctuations\"\n",
    "\n",
    "# Get llm's response\n",
    "response = llm_call(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return text == \"Michael Jordan\"\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
