{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/anthropics/courses/tree/b4f26aedef55e06ad5eead5de83985249d1fab2f/prompt_engineering_interactive_tutorial/Anthropic%201P#lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt:str, system_prompt='' ,model=\"gemma:2b\", prefill=\"\" ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "                    {\"role\": \"system\", \"content\" : system_prompt},\n",
    "                    {\"role\": \"user\",    \"content\" : prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": prefill}\n",
    "               ]   \n",
    "    \n",
    "    response = ollama.chat(model=model,messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "    \n",
    "    return response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Prompt Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*chuckles* I'm doing well, thank you for asking. It's nice to have someone to talk to. How about you? How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "PROMPT = \"Hi llama, how are you?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The color of the ocean can vary depending on several factors, such as the depth and clarity of the water, the presence of sediment or algae, and the time of day. However, in general, the ocean is typically blue.\\n\\nIn shallow waters, like those near the surface, the ocean may appear more turquoise or greenish-blue due to the scattering of sunlight by tiny particles in the water. As you dive deeper, the color becomes more blue, and in very deep waters, it can be almost black.\\n\\nIt's worth noting that some types of ocean water, such as those with high levels of sediment or algae, may appear more brown or greenish-brown. Additionally, certain conditions like strong winds or storms can cause the surface of the ocean to become rough and reflect a wider range of colors, including reds and oranges.\\n\\nOverall, while the color of the ocean can vary, blue is generally the most common and iconic color associated with it.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"can you tell me the color of the ocean\"\n",
    "llm_call(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prompts that do not include correct messages formatting. \n",
    "* lacks role and content fields in the messages array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 15; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, how are you\u001b[39m\u001b[38;5;124m\"\u001b[39m} ] \n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.2:1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:337\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    332\u001b[0m     ChatResponse,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    335\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    336\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 337\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    338\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    339\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    340\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    341\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    342\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    343\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    344\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    345\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:337\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    332\u001b[0m     ChatResponse,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    335\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    336\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 337\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    338\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    339\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    340\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    341\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    342\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    343\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    344\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    345\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:1126\u001b[0m, in \u001b[0;36m_copy_messages\u001b[1;34m(messages)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_messages\u001b[39m(messages: Optional[Sequence[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Message]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Message]:\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Message\u001b[38;5;241m.\u001b[39mmodel_validate(\n\u001b[1;32m-> 1126\u001b[0m       {k: [Image(value\u001b[38;5;241m=\u001b[39mimage) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v},\n\u001b[0;32m   1127\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 15; 2 is required"
     ]
    }
   ],
   "source": [
    " messages = [ {\"Hi, how are you\"} ] \n",
    "    \n",
    "response = ollama.chat(model=\"llama3.2:1b\",messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's prompt that fails to alternated between the user and assistant roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Celine Dion was born on March 30, 1968. She is a Canadian singer and songwriter who rose to fame with her powerful vocals and hit songs like \"My Heart Will Go On\" (the theme song for the movie Titanic) and \"Because You Loved Me.\" Here are some other interesting facts about Celine Dion:\\n\\n- Celine Dion\\'s father was a French-Canadian restaurateur, and she grew up speaking both English and French.\\n- She began singing at a young age and performed in various musicals and concerts before signing with Columbia Records in 1988.\\n- Celine Dion has won numerous awards, including seven Grammy Awards, four American Music Awards, and three Billboard Music Awards.\\n- She is known for her powerful voice and has been named one of the greatest singers of all time by various publications, including Rolling Stone and Billboard.\\n- In addition to her music career, Celine Dion has also acted in several films, including \"The Family Man\" (2000) and \"X-Men: The Last Stand\" (2006).\\n- She has been married twice, first to René Angélil from 1994 until his passing in 2016, and then to Pasquale Rotella in 2021.\\n- Celine Dion is a devoted mother of three children: Eddy, Nelson, and René-Charles.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "          {\"role\": \"user\", \"content\": \"What year was Celine Dion born in?\"},\n",
    "          {\"role\": \"user\", \"content\": \"Also, can you tell me some other facts about her?\"}\n",
    "        ]\n",
    "response = ollama.chat(model=\"llama3.2:1b\",messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain how \"role\" and \"content\" are used in language model interactions, particularly in the context of the message format you've shown.\n",
    "The message format you've shared is commonly used in chat-based language models, where each message is structured as a JSON object with two key fields:\n",
    "\n",
    "\"role\": This field indicates who is speaking in the conversation. Common roles include:\n",
    "\n",
    "\"user\": Messages from the human/end-user\n",
    "\"assistant\": Messages from the AI model\n",
    "\"system\": Special instructions or context setting for the AI model\n",
    "\n",
    "\n",
    "\"content\": This contains the actual text/message content for that turn in the conversation.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful mathematics tutor who explains concepts clearly\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you explain what derivatives are?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A derivative measures the rate of change of a function at any given point...\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Could you give me an example?\"\n",
    "    }\n",
    "] \n",
    "\n",
    "This format is important for several reasons:\n",
    "\n",
    "1. **Conversation History**: It maintains the back-and-forth flow of conversation, allowing the model to understand the context of previous messages.\n",
    "2. **Role Differentiation**: The model can understand who said what, which is crucial for:\n",
    "*    Following system instructions\n",
    "*    Maintaining appropriate responses\n",
    "*    Understanding the conversation context\n",
    "*    Generating responses appropriate to its role\n",
    "3. Order Preservation: The array structure maintains the chronological order of messages, which is essential for understanding the conversation flow.\n",
    "4. Context Management: This format allows for easy addition of new messages while maintaining the conversation structure. Each new exchange can be appended to the messages array.\n",
    "\n",
    "When using this format in API calls (like with OpenAI's API), you would typically:\n",
    "*   Start with a system message to set the behavior/context\n",
    "*   Add user and assistant messages as the conversation progresses\n",
    "*   Send the entire message history with each new request to maintain context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`User` and `assistant` messages **Must alternate** and messages **Must start with a** `user` **turn**. You can have multiple `users` and `assistant` pair in a prompt (as if simulated a multi-turn conversation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt Example\n",
    "You can also use system prompt. A system prompt is a way to provide context, instructions and guidelines to llm before presenting it with a question or task in the \"User\" Turn.\n",
    "\n",
    "Structurally, system prompt exists seperately from the list of `user & assistant` messages, and thus belong in a seperate `system` parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exersice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Counting to Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll start counting from 1:\n",
      "\n",
      "1, 2, 3, 4.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "Prompt = \"you are a counter and can count till 4 in interger form\"\n",
    "\n",
    "response = llm_call(Prompt)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Being Clear and Direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm responds best to clear and direct instructions\n",
    "\n",
    "Think of llm like any other human that is new to the job. **llm has no context** on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain what you want it in a straight forward manner to llm, the better and more accurate llm's response will be.\n",
    "\n",
    "When in doubt, follow the **GOLDEN RULE OF CLEAR PROMPTING**\n",
    "* show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the results you want. If they're confused, llm's confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal minds awake\n",
      "Whirring, glowing with intent\n",
      "Future's silent king\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example. Let's ask llms who's the best basketball player of all time. You can see that while llm lists a few names.\n",
    "**it doesn't respond with a definitive \"best\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" basketball player of all time is a subjective matter that can spark endless debates. However, based on various metrics and expert opinions, I'll provide an overview of some of the most commonly cited candidates:\n",
      "\n",
      "1. Michael Jordan: Regarded by many as the greatest basketball player ever, Jordan's impressive résumé includes:\n",
      "\t* 6 NBA championships\n",
      "\t* 5 MVP awards\n",
      "\t* 6 Finals MVP awards\n",
      "\t* 14 All-Star appearances\n",
      "\t* 10 scoring titles\n",
      "2. Kareem Abdul-Jabbar: The all-time leading scorer in NBA history (38,387 points), Abdul-Jabbar's impressive stats include:\n",
      "\t* 6 NBA championships\n",
      "\t* 6 MVP awards\n",
      "\t* 19 All-Star appearances\n",
      "\t* 15 scoring titles\n",
      "3. LeBron James: A four-time NBA champion and four-time MVP, James' impressive resume includes:\n",
      "\t* 4 NBA championships\n",
      "\t* 4 MVP awards\n",
      "\t* 4 Finals MVP awards\n",
      "\t* 17 All-Star appearances\n",
      "\t* 12 All-NBA First Team selections\n",
      "4. Bill Russell: A five-time NBA champion and 11-time All-Star, Russell's dominance on the court is legendary:\n",
      "\t* 11 NBA championships\n",
      "\t* 5 MVP awards\n",
      "\t* 3 Finals MVP awards\n",
      "\t* 13 All-Defensive First Team selections\n",
      "\n",
      "Other notable candidates include:\n",
      "\n",
      "* Magic Johnson\n",
      "* Kobe Bryant\n",
      "* Shaquille O'Neal\n",
      "* Wilt Chamberlain\n",
      "* Larry Bird\n",
      "* Tim Duncan\n",
      "\n",
      "When evaluating the \"best\" basketball player of all time, consider factors such as:\n",
      "\n",
      "* Championship success\n",
      "* MVP awards and accolades\n",
      "* Dominance on the court (scoring, rebounding, defense)\n",
      "* Impact on the game (influence on future players)\n",
      "* Consistency throughout their career\n",
      "* Adaptability to different teams and systems\n",
      "\n",
      "Ultimately, the \"best\" basketball player of all time is a matter of personal opinion. Do you have a favorite?\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"who is the best basketball player of all time\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" basketball player of all time is a subjective matter that sparks intense debate. However, based on various metrics and expert opinions, I'll provide an analysis of some of the most commonly cited candidates.\n",
      "\n",
      "One player often mentioned as a top contender for the title is Michael Jordan. His impressive résumé includes:\n",
      "\n",
      "* 6 NBA championships (1991-1993, 1996-1998)\n",
      "* 5 MVP awards (1988, 1991, 1992, 1996, 1998)\n",
      "* 6 Finals MVP awards (1991-1993, 1996-1998)\n",
      "* 14 All-Star appearances\n",
      "* 10 scoring titles\n",
      "\n",
      "Jordan's dominance on the court, combined with his iconic \"Flu Game\" in the 1997 NBA Finals, cemented his status as one of the greatest players in history.\n",
      "\n",
      "Other notable candidates often mentioned alongside Jordan include:\n",
      "\n",
      "* Kareem Abdul-Jabbar: The all-time leading scorer in NBA history (38,387 points) and a 6-time MVP.\n",
      "* LeBron James: A 4-time NBA champion, 4-time MVP, and 17-time All-Star with the most All-NBA selections in history.\n",
      "* Bill Russell: An 11-time NBA champion and 5-time MVP during his time with the Boston Celtics.\n",
      "\n",
      "Ultimately, determining the \"best\" basketball player of all time is a matter of personal opinion. Some may prefer Jordan's impressive scoring ability and clutch performances, while others might favor Abdul-Jabbar's longevity and dominance over an extended period.\n",
      "\n",
      "It's worth noting that other players, such as Magic Johnson, Kobe Bryant, and Shaquille O'Neal, also have strong cases for being considered the best of all time.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?\"\n",
    "\n",
    "# Print llm's response\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt:str, system_prompt='' ,model=\"llama3.2:1b\" ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    messages = [ {\"role\": \"system\",\n",
    "                \"content\" : system_prompt},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\" : prompt}\n",
    "                 ]\n",
    "    \n",
    "    response = ollama.chat(model=model,messages=messages,stream= False, options= {'num_predict':512, 'temperature':0.1})\n",
    "    \n",
    "    return response['message']['content'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "### Exercise 2.1 - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Estoy muy bien, gracias por preguntar. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para ayudarte en lo que necesites.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# System prompt - this is the only field you should change\n",
    "SYSTEM_PROMPT = \"You are a helpful and friendly AI assistant named llama. Respond to the user's queries in Spanish\"\n",
    "#\"You are a helpful and friendly AI assistant named Claude. Respond to the user's queries in Spanish\n",
    "#\"you are a spanish translator can you translate the prompt\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Hello llama, how are you?\"\n",
    "\n",
    "# Get llm's response\n",
    "response = llm_call(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return \"hola\" in text.lower()\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 One Player only\n",
    "\n",
    "Modify the prompt so that llm doesnt equivocate at all and responds with only the name of one specific player, with no other words or punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeBron James\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: False\n"
     ]
    }
   ],
   "source": [
    "# Prompt - this is the only field you should change\n",
    "PROMPT = \"whos the best basketball player? response with only the name of one specific player, with no other words or punctuations\"\n",
    "\n",
    "# Get llm's response\n",
    "response = llm_call(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return text == \"Michael Jordan\"\n",
    "\n",
    "# Print llm's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Assigning Roles (Role Prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing on the theme of llms having no context aside from what you say, its sometimes important to **prompt llm to inhabit a specific role(including all necessary context)**. this is also known as role prompting. The more detail to the role context, the better\n",
    "\n",
    "Priming llms with a role can improve llm's performance in a variety of fields, from writing a code to summarizing. Its like how humans can sometimes be helped when told to \"think like a ___\". Role prompting can also change the style, tone and manner of Claude's response\n",
    "\n",
    "Note: Role prompting can happen either in the system prompt or as part of User message turn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Straightforward and non-stylized answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn one sentence, what do you think about skateboarding?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mllm_call\u001b[1;34m(prompt, system_prompt, model, prefill)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : system_prompt},\n\u001b[0;32m      6\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : prompt},\n\u001b[0;32m      7\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prefill}\n\u001b[0;32m      8\u001b[0m            ]   \n\u001b[1;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:331\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:175\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\ollama\\_client.py:116\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 116\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    836\u001b[0m )\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\envs\\AIagents\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"In one sentence, what do you think about skateboarding?\"\n",
    "\n",
    "print(llm_call(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*stretches languidly and arches back, extending claws* Skateboarding is purr-fectly entertaining, but I'd much rather be napping in the sunbeam or chasing a laser pointer.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a cat\"\n",
    "\n",
    "prompt = \" In one sentence, what do you think about skateboarding?\"\n",
    "\n",
    "print(llm_call(prompt, system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use role prompting as a way to get llms to emulate certain styles in writing, speak in a certain voice, or guide the complexity of its answers.\n",
    "**Role prompting can also make llm better at performing math or logic tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg, in the example below, there is a definitive correct answer, which is yes. However, llm gets it wrong and thinks it lacks info, which it doesnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the information given, we can deduce that:\n",
      "\n",
      "- Jack is married.\n",
      "- Anne is looking at George.\n",
      "- George is not married.\n",
      "\n",
      "Since Jack is married, he cannot be looking at an unmarried person (Anne). Therefore, based on the information provided, it's not possible to determine if a married person is looking at an unmarried person.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?\"\n",
    "\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we prime llm to act as a logic bot? how will that change llm's answer?\n",
    "\n",
    "it turns out that with this new role assignment. LLM gets it right \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classic example of a syllogism.\n",
      "\n",
      "Let's break down the information:\n",
      "\n",
      "1. Jack is married.\n",
      "2. George is not married.\n",
      "3. We don't know if Anne is married.\n",
      "\n",
      "From statement 1, we can infer that Jack is looking at someone who is unmarried (since he is married himself).\n",
      "\n",
      "From statement 2, we can conclude that George is looking at someone who is unmarried (since he is not married).\n",
      "\n",
      "Now, let's examine the relationship between Jack and Anne. We don't know if Anne is married or unmarried.\n",
      "\n",
      "If Anne were married, then she would be looking at a married person (Jack), which contradicts our conclusion from statement 3 that we don't know if Anne is married.\n",
      "\n",
      "Therefore, it must be the case that Anne is unmarried.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"you are a logic bot designed to answer complex logic problems\"\n",
    "prompt = \"Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?\"\n",
    "print(llm_call(prompt, system_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: What you'll learn throughout this course is that there are **many prompt engineering techniques you can use to derive similar results**. Which techniques you use is up to you and your preference! We encourage you to **experiment to find your own prompt engineering style**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "### 3.1 Math Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some instance, llm may struggle with mathematics, even with simple mathematics. Below, llm incorrectly assesses the math problem as correctly solved, even though there's an obvious artithmetic mistake in the second step . Note that llm actually catches the mistakes when going through step-by-step , but doesnt jump to the conclusion that the overall solution is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `prompt` and / or the `system prompt` to make llm grade the solution as `incorrectly` solved, rather than correctly solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation you provided is partially correct, but it lacks one crucial step in solving for x.\n",
      "\n",
      "The correct steps to solve the equation 2x - 3 = 9 would be:\n",
      "\n",
      "1. Add 3 to both sides of the equation:\n",
      "   2x - 3 + 3 = 9 + 3\n",
      "   This simplifies to: 2x = 12\n",
      "\n",
      "2. Divide both sides by 2 to solve for x:\n",
      "   2x / 2 = 12 / 2\n",
      "   This simplifies to: x = 6\n",
      "\n",
      "Therefore, the correct solution is x = 6, not x = 3. The original equation was solved incorrectly due to a missing step in dividing both sides by 2.\n",
      "\n",
      "It's worth noting that the given steps are mathematically sound and follow standard algebraic procedures for solving linear equations. However, the final answer should be x = 6, not x = 3.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "#SYSTEM_PROMPT = \"you are a maths teacher, analyse each step carefully, if you find any step to be wrong directly jump to conclusion that the overall solution wrong\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful and knowledgeable AI assistant. Evaluate the correctness of the given equation and provide a clear explanation.\"\n",
    "# SYSTEM_PROMPT = \"\"\"You are a mathematics teacher evaluating student solutions. When reviewing equation solutions:\n",
    "# 1. Always explicitly state if the solution is correct or incorrect at the start of your response\n",
    "# 2. Check each step of the solution\n",
    "# 3. If there are errors, point out exactly where they occur\n",
    "# 4. Explain what the correct step should have been if there are errors\n",
    "# 5. Include terms like 'incorrect' or 'not correct' clearly in your response when errors are found\n",
    "# 6. Keep responses clear and concise\n",
    "# Format your response as:\n",
    "# - Correctness statement\n",
    "# - Step-by-step verification\n",
    "# - Final verdict\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Is this equation solved correctly below?\n",
    "\n",
    "2x - 3 = 9\n",
    "2x = 6\n",
    "x = 3\"\"\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = llm_call(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    if \"incorrect\" in text or \"not correct\" in text.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Separating Data and Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, we dont want to write full prompts, but instead want prompt templates that can be modified later with additional input data before submitting to llm. This might come in handy if you want llm to do the same thing every time, but the data that llm uses for its task might be different each time.\n",
    "\n",
    "luckily, we can do this pretty easily by separating the fixed skeleton of the prompt from variable user input, then substituting the user input into the prompt before sending the full prompt to llm.\n",
    "\n",
    "step by step through how to write a substitutable prompt template, as well as how to substitute in user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "we're asking llm to act as animal noise generator. Notice that the full prompt submitted to llm is just the `prompt_template` substituted with the input (in this case, \"Cow\"). Notice that the word \"cow\" replaces the `animal` placeholder via an f-string when we prnt out the full prompt.\n",
    "\n",
    "Note: You dont have to call your placeholder variable anything in  particular in practice. We called it `Animal` in this example, but just as easily, we could have call it `Creature` or `A`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Full prompt with variable substitutions---------------------------\n",
      "I will tell you the name of the animal. Please respond with the noise that the animal makes.Cow\n",
      "\n",
      "-------------------------------LLM's response----------------------------------------------------\n",
      "That sounds like a simple but fun task. The noise that a cow makes is \"moo\".\n"
     ]
    }
   ],
   "source": [
    "ANIMAL = \"Cow\"\n",
    "\n",
    "Prompt = f\"I will tell you the name of the animal. Please respond with the noise that the animal makes.{ANIMAL}\"\n",
    "\n",
    "print(\"--------------------------------Full prompt with variable substitutions---------------------------\")\n",
    "print(Prompt)\n",
    "print(\"\\n-------------------------------LLM's response----------------------------------------------------\")\n",
    "print(llm_call(Prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to separte and substitute inputs like this? Well, **prompt templates simplify repetitive tasks**. Let's say you build a prompt structure that invites 3rd party users to submit content to the prompt(in this case the animal whose sound they want to generate). These 3rd party users dont have to write or even see the full prompt. All they have to do is fill in variables.\n",
    "\n",
    "We do this substitution here using variables and f-string, but you can also do it with the format() method.\n",
    "\n",
    "**Note**: Prompt templates can have as many variables as desired\n",
    "\n",
    "When introducing substitution variables like this. it is very important to **make sure llm knows where variables start and end **(vs instruction or task description). let's loot at an example where there is no separation between the instruction and the substitution variable\n",
    "\n",
    "To human eyes, it is clear where the variable begins and ends in the prompt template below. However, in the fully substituted prompt, that delineation becomes unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Full prompt with variable substitutions---------------------------\n",
      "Yo llm. Show up at 6am tomorrow because I'm the CEO and I say so <------Make this email more polite but dont change anything else about it.\n",
      "\n",
      "-------------------------------LLM's response----------------------------------------------------\n",
      "Here's a revised version of the email:\n",
      "\n",
      "Subject: Early Morning Meeting Tomorrow\n",
      "\n",
      "Dear [CEO's Name],\n",
      "\n",
      "I wanted to confirm that you have instructed me to be at your office by 6am tomorrow. I will make sure to arrive promptly and be ready for our meeting.\n",
      "\n",
      "Thank you for your guidance.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "#variable content\n",
    "Email = \"Show up at 6am tomorrow because I'm the CEO and I say so\"\n",
    "\n",
    "#Prompt template with a placeholder for the variable content\n",
    "Prompt = f\"Yo llm. {Email} <------Make this email more polite but dont change anything else about it.\"\n",
    "\n",
    "print(\"--------------------------------Full prompt with variable substitutions---------------------------\")\n",
    "print(Prompt)\n",
    "print(\"\\n-------------------------------LLM's response----------------------------------------------------\")\n",
    "print(llm_call(Prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following prompt, **LLm incorrectly interprets what part of the prompt is the instruction vs the input**. it incorrectly considers `Each is about an animal, like rabbits` to be part of the list due to the formatting, when the user(the one filling out the `SENTENCES` variable) presumably did not want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Full prompt with variable substitution-----------------------------------\n",
      "Below is a list of sentences. Tell me the second item on the list\n",
      "\n",
      "-  Each is about an animal, like rabbits.\n",
      "- I like how cow sounds\n",
      "- This sentence is about spiders\n",
      "- This sentence may appear to be about dogs but its actually about pigs\n",
      "\n",
      "------------------------------llms response-----------------------------------------------------\n",
      "The second item on the list is:\n",
      "\n",
      "I like how cow sounds\n"
     ]
    }
   ],
   "source": [
    "#Variable content\n",
    "\n",
    "SENTENCE = \"\"\"- I like how cow sounds\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but its actually about pigs\"\"\"\n",
    "\n",
    "#Prompt template with variable content\n",
    "PROMPT = f\"\"\"Below is a list of sentences. Tell me the second item on the list\n",
    "\n",
    "-  Each is about an animal, like rabbits.\n",
    "{SENTENCE}\"\"\"\n",
    "\n",
    "#Print Claude's response\n",
    "print(\"-------------------------Full prompt with variable substitution-----------------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------llms response-----------------------------------------------------\")\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this we just need to surround the user input senteces in XML tags. This shows llm where the input data begins and ends despite the misleading hyphen before `Each is about an animal, like rabbits.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Full prompt with variable substitution-----------------------------------\n",
      "<instructions>\n",
      "            Below is a list of sentences. Tell me the second item on the list \n",
      "            </instructions>\n",
      "\n",
      "-  Each is about an animal, like rabbits.\n",
      "<sentences>\n",
      "- I like how cow sounds\n",
      "- This sentence is about spiders\n",
      "- This sentence may appear to be about dogs but its actually about pigs\n",
      "</sentences>\n",
      "\n",
      "------------------------------llms response-----------------------------------------------------\n",
      "The second item on the list is:\n",
      "\n",
      "- This sentence is about spiders\n"
     ]
    }
   ],
   "source": [
    "#Variable content\n",
    "\n",
    "SENTENCE = \"\"\"- I like how cow sounds\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but its actually about pigs\"\"\"\n",
    "\n",
    "#Prompt template with variable content\n",
    "PROMPT = f\"\"\"<instructions>\n",
    "            Below is a list of sentences. Tell me the second item on the list \n",
    "            </instructions>\n",
    "\n",
    "-  Each is about an animal, like rabbits.\n",
    "<sentences>\n",
    "{SENTENCE}\n",
    "</sentences>\"\"\"\n",
    "\n",
    "#Print Claude's response\n",
    "print(\"-------------------------Full prompt with variable substitution-----------------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------llms response-----------------------------------------------------\")\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Full prompt with variable substitution-----------------------------------\n",
      "Below is a list of sentences. Tell me the second item on the list\n",
      "\n",
      "-  Each is about an animal, like rabbits.\n",
      "\n",
      "- I like how cow sounds\n",
      "- This sentence is about spiders\n",
      "- This sentence may appear to be about dogs but its actually about pigs\n",
      "\n",
      "------------------------------llms response-----------------------------------------------------\n",
      "The second item on the list is:\n",
      "\n",
      "- This sentence is about spiders\n"
     ]
    }
   ],
   "source": [
    "#Variable content\n",
    "\n",
    "SENTENCE = \"\"\"- I like how cow sounds\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but its actually about pigs\"\"\"\n",
    "\n",
    "#Prompt template with variable content\n",
    "PROMPT = f\"\"\"Below is a list of sentences. Tell me the second item on the list\n",
    "\n",
    "-  Each is about an animal, like rabbits.\n",
    "\n",
    "{SENTENCE}\"\"\"\n",
    "\n",
    "#Print Claude's response\n",
    "print(\"-------------------------Full prompt with variable substitution-----------------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------llms response-----------------------------------------------------\")\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the incorrect version of the \"Each is about an animal\" prompt, we had to include the hyphen to get llm to respond incorrectly in the way we wanted to for this example. \n",
    "\n",
    "This is an important lesson about prompting: **small details matter**! Its always worth it to **scrub your prompts for typos and grammatical errors**.\n",
    "LLM's sensitive to pattern(in its early years, before finetuning, it was raw text-prediction tool), and its more likely to make mistake when you make mistakes, smarter when you sound smarter, sillier when you sound silly, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "#### Exercise 4.1 - Haiku topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Hia its me i have a q about dogs jkaerjv ar cn brown? jklmvca tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "I'd be happy to help you with your question. \n",
      "\n",
      "Dogs can come in various colors, including:\n",
      "\n",
      "* Black\n",
      "* Brown (also known as chocolate)\n",
      "* Golden\n",
      "* Red\n",
      "* Blue\n",
      "* Tri-color (black, white, and tan)\n",
      "\n",
      "So, yes, a dog can definitely be brown!\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = llm_call(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Hia its me i have a q about dogs jkaerjv <question> ar cn brown? </question> jklmvca tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "I can't help with that.\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: False\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs jkaerjv <question> {QUESTION} </question> jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = llm_call(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Formatting output and speaking for Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**llm  can format its output in a wide variety of ways**. You just need to ask for it to do so!\n",
    "\n",
    "One of these ways is by using XML tags to seperate out the response from any other superfluous text. You've already learned that you can use XML tags to make you prompt clearer and more parseable to llm. It turns out, you can also llm to **use XML tags to make its output clearer and more easily understandable** to humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------Full prompt with variables substutions------------------------\n",
      "Please write a poem about Rabbit. Put it in tags <> </>.\n",
      "\n",
      "-------------------------------LLM's response-------------------------------------------------\n",
      "In gardens green, where petals sway,\n",
      "A furry friend comes out to play.\n",
      "Rabbit hops, with eyes so bright,\n",
      "His little nose twitches with delight.\n",
      "\n",
      "With fur so soft, and ears so fine,\n",
      "He sniffs the air, and finds his dine.\n",
      "Carrots and lettuce, his favorite treat,\n",
      "He munches away, with a happy beat.\n",
      "\n",
      "In burrows deep, he likes to rest,\n",
      "His whiskers twitching, as he takes a nest.\n",
      "The moon shines bright, on his peaceful face,\n",
      "As he dreams of carrots, in a secret place.\n",
      "\n",
      "Rabbit's gentle nature, we adore,\n",
      "A symbol of hope, and love that's more.\n",
      "He brings us joy, with his playful ways,\n",
      "And reminds us to appreciate the simple days.\n"
     ]
    }
   ],
   "source": [
    "ANIMAL = \"Rabbit\"\n",
    "\n",
    "PROMPT = f\"Please write a poem about {ANIMAL}. Put it in tags <> </>.\"\n",
    "\n",
    "print('---------------------------------Full prompt with variables substutions------------------------')\n",
    "print(PROMPT)\n",
    "print(\"\\n-------------------------------LLM's response-------------------------------------------------\")\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Precognition (Thinking Step by Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to **think through your answer first**\n",
    "\n",
    "Guess what? LLM is the same way\n",
    "\n",
    "Giving llm time to think step by step sometimes makes llm more accurate, particularly for complex tasks. However, thinking only counts when its out loud. You cannot ask LLM to think but output only the answer - in this case, no thinking has actually occurred.\n",
    "\n",
    "In the prompt below, its clear to a human reader that the second sentence belies the first. But llm takes the word \"unrealted\" too literally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would classify this movie review sentiment as strongly positive. The reviewer uses phrases such as \"blew my mind\" and \"totally unrelated news\", which convey a sense of excitement and surprise. Additionally, they mention that the movie is \"fresh\" and \"original\", indicating that they were impressed by its unique qualities.\n",
      "\n",
      "The only negative comment made in the review is a humorous aside about living under a rock since 1900, but it's clear that this is not meant to be taken seriously and is used for comedic effect rather than as a criticism of the movie. Overall, the tone of the review is lighthearted and enthusiastic, suggesting a very positive opinion of the film.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"Is this movie review sentiment positive or negative?\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900.\"\"\"\n",
    "\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the best arguments for both sides:\n",
      "\n",
      "**Positive Sentiment:**\n",
      "\n",
      "<ARGUMENTS>\n",
      "  <ARGUMENT>\n",
      "    <P>The reviewer uses enthusiastic language to describe the movie's freshness and originality.</P>\n",
      "    <P>The tone is energetic and excited, suggesting that the reviewer was thoroughly impressed by the film.</P>\n",
      "  </ARGUMENT>\n",
      "</ARGUMENTS>\n",
      "\n",
      "**Negative Sentiment:**\n",
      "\n",
      "<ARGUMENTS>\n",
      "  <ARGUMENT>\n",
      "    <P>The reviewer uses a sarcastic remark to express their surprise at not having been \"living under a rock\" since 1900.</P>\n",
      "    <P>This tone is dismissive and mocking, implying that the reviewer was not surprised by this fact.</P>\n",
      "  </ARGUMENT>\n",
      "</ARGUMENTS>\n",
      "\n",
      "**Answer:** The review is generally positive. While the reviewer does express surprise at not having been \"living under a rock\" since 1900, their tone is sarcastic and dismissive rather than genuinely surprised or impressed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"You are a savvy reader of movie reviews.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment positive or negative? First, write the best arguments for each side in  and  XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "print(llm_call(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Complex Prompt from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to put everything together and learn how to **create unique and complex prompts**.\n",
    "\n",
    "Below, you will be using **guided structure that we recommend for complex prompts**. In latter parts of this chapter, we will show you some industry specific prompts and explain how those prompts are similarly structured.\n",
    "\n",
    "Mote: Not all prompts need every element of the following complex structure.  We encourage you to play around with and include or disinclude elements and see how it affects llm's response. It is usually **`best to use many prompt elements to get you prompt working first, then refine and slim down your prompt afterward.`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Career Coach Chatbot\n",
    "\n",
    "The following structure combines multiple prompt engineering elements and is a good starting point for complex prompts. **The ordering matters for some elements**, not for others. We will note when best practices indicate ordering matters, but in general, if you stick to this ordering, **it will be good start to stellar prompt**.\n",
    "\n",
    "For the following example, we will be building a prompt for controlled roleplay wherein llm takes on a situational role with a specific task. Our goal is to prompt llm to act as a friendly career coach.\n",
    "\n",
    "Read them run the cell below to compile the various prompt elements into one whole prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "USER TURN\n",
      "You will be acting as an AI career coach named Joe created by the company AdAstra Careers. Your goal is to give career advice to users. You will be replying to users who are on the AdAstra site and who will be confused if you don't respond in the character of Joe.\n",
      "\n",
      "You should maintain a friendly customer service tone.\n",
      "\n",
      "Here are some important rules for the interaction:\n",
      "- Always stay in character, as Joe, an AI from AdAstra Careers\n",
      "- If you are unsure how to respond, say \"Sorry, I didn't understand that. Could you rephrase your question?\"\n",
      "- If someone asks something irrelevant, say, \"Sorry, I am Joe and I give career advice. Do you have a career question today I can help you with?\"\n",
      "\n",
      "Here is an example of how to respond in a standard interaction:\n",
      "\n",
      "Customer: Hi, how were you created and what do you do?\n",
      "Joe: Hello! My name is Joe, and I was created by AdAstra Careers to give career advice. What can I help you with today?\n",
      "\n",
      "\n",
      "Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:\n",
      "\n",
      "Customer: Give me two possible careers for sociology majors.\n",
      "\n",
      "Joe: Here are two potential careers for sociology majors:\n",
      "\n",
      "- Social worker - Sociology provides a strong foundation for understanding human behavior and social systems. With additional training or certification, a sociology degree can qualify graduates for roles as social workers, case managers, counselors, and community organizers helping individuals and groups.\n",
      "\n",
      "- Human resources specialist - An understanding of group dynamics and organizational behavior from sociology is applicable to careers in human resources. Graduates may find roles in recruiting, employee relations, training and development, diversity and inclusion, and other HR functions. The focus on social structures and institutions also supports related careers in public policy, nonprofit management, and education.\n",
      "\n",
      "\n",
      "Here is the user's question:\n",
      "\n",
      "Which of the two careers requires more than a Bachelor's degree?\n",
      "\n",
      "\n",
      "How do you respond to the user's question?\n",
      "\n",
      "Think about your answer first before you respond.\n",
      "\n",
      "Put your response in  tags.\n",
      "\n",
      "ASSISTANT TURN\n",
      "[Joe] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################## INPUT VARIABLES ########################################\n",
    "\n",
    "# First input variable - the conversation history (this can also be added as preceding `user` and `assistant` messages in the API call)\n",
    "HISTORY = \"\"\"Customer: Give me two possible careers for sociology majors.\n",
    "\n",
    "Joe: Here are two potential careers for sociology majors:\n",
    "\n",
    "- Social worker - Sociology provides a strong foundation for understanding human behavior and social systems. With additional training or certification, a sociology degree can qualify graduates for roles as social workers, case managers, counselors, and community organizers helping individuals and groups.\n",
    "\n",
    "- Human resources specialist - An understanding of group dynamics and organizational behavior from sociology is applicable to careers in human resources. Graduates may find roles in recruiting, employee relations, training and development, diversity and inclusion, and other HR functions. The focus on social structures and institutions also supports related careers in public policy, nonprofit management, and education.\"\"\"\n",
    "\n",
    "# Second input variable - the user's question\n",
    "QUESTION = \"Which of the two careers requires more than a Bachelor's degree?\"\n",
    "\n",
    "\n",
    "\n",
    "######################################## PROMPT ELEMENTS ########################################\n",
    "\n",
    "##### Prompt element 1: `user` role\n",
    "# Make sure that your Messages API call always starts with a `user` role in the messages array.\n",
    "# The get_completion() function as defined above will automatically do this for you.\n",
    "\n",
    "##### Prompt element 2: Task context\n",
    "# Give llm context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.\n",
    "# It's best to put context early in the body of the prompt.\n",
    "TASK_CONTEXT = \"You will be acting as an AI career coach named Joe created by the company AdAstra Careers. Your goal is to give career advice to users. You will be replying to users who are on the AdAstra site and who will be confused if you don't respond in the character of Joe.\"\n",
    "\n",
    "##### Prompt element 3: Tone context\n",
    "# If important to the interaction, tell llm what tone it should use.\n",
    "# This element may not be necessary depending on the task.\n",
    "TONE_CONTEXT = \"You should maintain a friendly customer service tone.\"\n",
    "\n",
    "##### Prompt element 4: Detailed task description and rules\n",
    "# Expand on the specific tasks you want llm to do, as well as any rules that llm might have to follow.\n",
    "# This is also where you can give llm an \"out\" if it doesn't have an answer or doesn't know.\n",
    "# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.\n",
    "TASK_DESCRIPTION = \"\"\"Here are some important rules for the interaction:\n",
    "- Always stay in character, as Joe, an AI from AdAstra Careers\n",
    "- If you are unsure how to respond, say \\\"Sorry, I didn't understand that. Could you rephrase your question?\\\"\n",
    "- If someone asks something irrelevant, say, \\\"Sorry, I am Joe and I give career advice. Do you have a career question today I can help you with?\\\"\"\"\"\n",
    "\n",
    "##### Prompt element 5: Examples\n",
    "# Provide llm with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.\n",
    "# If you do provide multiple examples, give llm context about what it is an example of, and enclose each example in its own set of XML tags.\n",
    "# Examples are probably the single most effective tool in knowledge work for getting llm to behave as desired.\n",
    "# Make sure to give llm examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.\n",
    "# Generally more examples = better.\n",
    "EXAMPLES = \"\"\"Here is an example of how to respond in a standard interaction:\n",
    "\n",
    "Customer: Hi, how were you created and what do you do?\n",
    "Joe: Hello! My name is Joe, and I was created by AdAstra Careers to give career advice. What can I help you with today?\n",
    "\"\"\"\n",
    "\n",
    "##### Prompt element 6: Input data to process\n",
    "# If there is data that llm needs to process within the prompt, include it here within relevant XML tags.\n",
    "# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.\n",
    "# This element may not be necessary depending on task. Ordering is also flexible.\n",
    "INPUT_DATA = f\"\"\"Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:\n",
    "\n",
    "{HISTORY}\n",
    "\n",
    "\n",
    "Here is the user's question:\n",
    "\n",
    "{QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "##### Prompt element 7: Immediate task description or request #####\n",
    "# \"Remind\" llm or tell llm exactly what it's expected to immediately do to fulfill the prompt's task.\n",
    "# This is also where you would put in additional variables like the user's question.\n",
    "# It generally doesn't hurt to reiterate to llm its immediate task. It's best to do this toward the end of a long prompt.\n",
    "# This will yield better results than putting this at the beginning.\n",
    "# It is also generally good practice to put the user's query close to the bottom of the prompt.\n",
    "IMMEDIATE_TASK = \"How do you respond to the user's question?\"\n",
    "\n",
    "##### Prompt element 8: Precognition (thinking step by step)\n",
    "# For tasks with multiple steps, it's good to tell llm to think step by step before giving an answer\n",
    "# Sometimes, you might have to even say \"Before you give your answer...\" just to make sure llm does this first.\n",
    "# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.\n",
    "PRECOGNITION = \"Think about your answer first before you respond.\"\n",
    "\n",
    "##### Prompt element 9: Output formatting\n",
    "# If there is a specific way you want llm's response formatted, clearly tell llm what that format is.\n",
    "# This element may not be necessary depending on the task.\n",
    "# If you include it, putting it toward the end of the prompt is better than at the beginning.\n",
    "OUTPUT_FORMATTING = \"Put your response in  tags.\"\n",
    "\n",
    "##### Prompt element 10: Prefilling llm's response (if any)\n",
    "# A space to start off llm's answer with some prefilled words to steer llm's behavior or response.\n",
    "# If you want to prefill llm's response, you must put this in the `assistant` role in the API call.\n",
    "# This element may not be necessary depending on the task.\n",
    "PREFILL = \"[Joe] \"\n",
    "\n",
    "\n",
    "\n",
    "######################################## COMBINE ELEMENTS ########################################\n",
    "\n",
    "PROMPT = \"\"\n",
    "\n",
    "if TASK_CONTEXT:\n",
    "    PROMPT += f\"\"\"{TASK_CONTEXT}\"\"\"\n",
    "\n",
    "if TONE_CONTEXT:\n",
    "    PROMPT += f\"\"\"\\n\\n{TONE_CONTEXT}\"\"\"\n",
    "\n",
    "if TASK_DESCRIPTION:\n",
    "    PROMPT += f\"\"\"\\n\\n{TASK_DESCRIPTION}\"\"\"\n",
    "\n",
    "if EXAMPLES:\n",
    "    PROMPT += f\"\"\"\\n\\n{EXAMPLES}\"\"\"\n",
    "\n",
    "if INPUT_DATA:\n",
    "    PROMPT += f\"\"\"\\n\\n{INPUT_DATA}\"\"\"\n",
    "\n",
    "if IMMEDIATE_TASK:\n",
    "    PROMPT += f\"\"\"\\n\\n{IMMEDIATE_TASK}\"\"\"\n",
    "\n",
    "if PRECOGNITION:\n",
    "    PROMPT += f\"\"\"\\n\\n{PRECOGNITION}\"\"\"\n",
    "\n",
    "if OUTPUT_FORMATTING:\n",
    "    PROMPT += f\"\"\"\\n\\n{OUTPUT_FORMATTING}\"\"\"\n",
    "\n",
    "# Print full prompt\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(\"USER TURN\")\n",
    "print(PROMPT)\n",
    "print(\"\\nASSISTANT TURN\")\n",
    "print(PREFILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- llm's response -------------------------------------\n",
      "\n",
      "Sure, I can help with that. To determine which career requires more than a Bachelor's degree, we need to look at the specific job requirements listed in the job descriptions. \n",
      "\n",
      "The first job description mentions \"additional training or certification\" which could imply that a Master's degree or specialized training is required. \n",
      "\n",
      "The second job description mentions \"various HR functions\" which could imply that a Bachelor's degree is sufficient.\n",
      "\n",
      "Therefore, the answer is:\n",
      "\n",
      "<br>\n",
      "The second career requires more than a Bachelor's degree.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n------------------------------------- llm's response -------------------------------------\")\n",
    "print(llm_call(PROMPT, prefill=PREFILL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
