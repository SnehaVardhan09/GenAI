{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1 - **Building a LLM** \n",
    "1. Data preparation & sampling\n",
    "2. ***Attention mechanism*** <----------------\n",
    "3. LLM architecture\n",
    "\n",
    "4. Pretraining\n",
    "\n",
    "#### Stage 2 - **Foundation model**\n",
    "5. Training loop \n",
    "6. Model evaluation\n",
    "7. Load pretrained weights\n",
    "\n",
    "8. Finetuning  -> dataset with class labels - classifier\n",
    "              |-> instruction dataset - personal assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simplified self-attention: A simplified self-attention technique to introduce the broader idea.\n",
    "2. Self - attention: Self-attention with trainable weights that form the basis of the mechanism used in LLMs.\n",
    "3. Causal attention: A type of self-attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, ensuring temporal order during the text generation.\n",
    "4. Multi-head attention: An extension of self-attention and casual attention that enables the model to simultaneously attend to info from different rep subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem with modeling long sequence**\n",
    "- Translating a text word by word isn't feasible due to the difference in grammatical structures between the souce and target languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder processes a sequence of tokens from the source language, using a hidden state-- a kind of intermediate layer within the neural network -- to generate a condensed representation of the entire input sequence.\n",
    "- Through an attention mechanism, the text-generating decoder segment of the network is capable of selectively accessing all the input tokens, implying that certain input tokens hold more significance than others in the generation of specific output token\n",
    "- Self-attention in transformers is a technique designed to enhance input representations by enabling each position in a sequence to engage with and determine the relevance of every other position within the same sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple attention mechanism\n",
    "- Suppose an input sequence x(1) to x(T), x(1) is a d-dimensional vector rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "inputs = torch.tensor([[0.43, 0.15, 0.89],    #Your\n",
    "                       [0.55, 0.87, 0.66],    #journey\n",
    "                       [0.57, 0.85, 0.64],    #Starts\n",
    "                       [0.22, 0.58, 0.33],    #with\n",
    "                       [0.77, 0.25, 0.10],    #one\n",
    "                       [0.05, 0.80, 0.55]])   #step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
